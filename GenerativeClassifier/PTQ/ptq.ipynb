{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d9b1799-848f-46ab-a4f6-af89b29ff93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104f7b77-ecf1-4709-8b4e-e214a82d7e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Gen, GenQLSTM, GenQLSTM_2bit, GenQLSTM_1bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3496e90-4ad8-46e5-919d-2dd660f6a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = torch.load(os.path.join('data', 'ag_news', 'data', 'traindata.v40000.l80.s5000'))\n",
    "traindata = data_dict['traindata']\n",
    "trainlabel = data_dict['trainlabel']\n",
    "validdata = data_dict['validdata']\n",
    "validlabel = data_dict['validlabel']\n",
    "testdata = data_dict['testdata']\n",
    "testlabel = data_dict['testlabel']\n",
    "vocab_size = data_dict['vocabsize']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f581672-3a31-42f0-bd3d-b5ce0da8ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "word_emb_dim = 100  # size of word embeddings\n",
    "label_emb_dim = 100  # size of label embeddings\n",
    "hid_dim = 100  # number of hidden units\n",
    "nlayers = 1  # number of lstm layers\n",
    "nclass = 4  # number of classes\n",
    "dropout = 0\n",
    "use_cuda = torch.cuda.is_available()\n",
    "tied = False\n",
    "use_bias = False\n",
    "concat_label = 'hidden'\n",
    "avg_loss = False\n",
    "one_hot = False\n",
    "bit_width=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d914de-5558-42e9-8d0e-aa3f7a3b222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e2b79eb-c68c-4fe0-83ce-7f9179a33d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Gen(vocab_size, word_emb_dim, label_emb_dim, hid_dim, nlayers, nclass, dropout, use_cuda, tied, use_bias, concat_label, avg_loss, one_hot, bit_width).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c11d774-0d58-4f01-93d3-cc55053cdff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(reduce=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c4f2755-2382-4b9b-a380-ab4e55e7c4b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = 'ModelParameterLSTM_FP.pth'\n",
    "\n",
    "# Load the state dictionary from the file\n",
    "state_dict = torch.load(model_path)\n",
    "\n",
    "# Load the state dictionary into the model\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6737c08a-5b0c-4af0-92fc-ca5d165fd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batches(data, label, batch_size, is_eval = False, nclass = 0):\n",
    "    d_l = list(zip(data, label))\n",
    "    random.shuffle(d_l)\n",
    "    data, label = zip(*d_l)\n",
    "    data, label = list(data), list(label)\n",
    "\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        sentences = data[i:i + batch_size]\n",
    "        labels = label[i:i + batch_size]\n",
    "\n",
    "        s_l = zip(sentences, labels)\n",
    "        s_l = sorted(s_l, key = lambda l: len(l[0]), reverse=True)\n",
    "\n",
    "        sentences, labels = zip(*s_l)\n",
    "\n",
    "        sentences = list(sentences)\n",
    "        labels = list(labels)\n",
    "\n",
    "        # x_pred: predicted ground truth, padding in the end\n",
    "        # y_ext: extend label to the length of sentence length for concatnation\n",
    "        # seq_len: pred_seq_len = actual seq len - 1\n",
    "\n",
    "        y_ext = []\n",
    "        for idx, d in enumerate(sentences):\n",
    "            y_ext.append([labels[idx]] * (len(d) - 1))\n",
    "\n",
    "        if is_eval:\n",
    "            y_exts = []\n",
    "            for y_label in range(nclass):\n",
    "                y_ext = []\n",
    "                for d in sentences:\n",
    "                    y_ext.append(torch.LongTensor([y_label] * (len(d) - 1)))\n",
    "                y_exts.append(y_ext)\n",
    "\n",
    "            yield [torch.LongTensor(s) for s in sentences], y_exts, torch.LongTensor(labels)\n",
    "        else:\n",
    "            yield [torch.LongTensor(s) for s in sentences], \\\n",
    "                [torch.LongTensor(y) for y in y_ext], torch.LongTensor(labels)\n",
    "\n",
    "def evaluate(validdata, validlabel, model, criterion, args):\n",
    "    # Turn on evaluation mode which disables dropout.\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    total_correct = 0.\n",
    "\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for sents, y_exts, labels in batches(validdata, validlabel, args.batch_size, True, args.nclass):\n",
    "            hidden = model.init_hidden(len(sents))\n",
    "\n",
    "            x = nn.utils.rnn.pack_sequence([s[:-1] for s in sents])\n",
    "            x_pred = nn.utils.rnn.pack_sequence([s[1:] for s in sents])\n",
    "\n",
    "            # p_y = torch.FloatTensor([0.071] * len(seq_len))\n",
    "\n",
    "            losses = []\n",
    "            for y_ext in y_exts:\n",
    "                y_ext = nn.utils.rnn.pack_sequence(y_ext)\n",
    "\n",
    "                if args.device.type == 'cuda':\n",
    "                    x, y_ext, x_pred, labels = x.cuda(), y_ext.cuda(), x_pred.cuda(), labels.cuda()\n",
    "\n",
    "                # output (batch_size, )\n",
    "                hidden = model.init_hidden(len(sents))\n",
    "                #if args.device.type == 'cuda':\n",
    "                #    hidden = hidden.cuda()\n",
    "                #    model=model.cuda()\n",
    "                loss = model(x, x_pred, y_ext, hidden, criterion, True)\n",
    "                losses.append(loss)\n",
    "\n",
    "            losses = torch.cat(losses, dim=0).view(-1, len(sents))\n",
    "            prediction = torch.argmin(losses, dim=0)\n",
    "\n",
    "            num_correct = (prediction == labels).float().sum()\n",
    "\n",
    "            total_loss += torch.sum(torch.min(losses, dim=0)[0]).item()\n",
    "            total_correct += num_correct.item()\n",
    "            cnt += 1\n",
    "\n",
    "    return total_loss / cnt, total_correct / len(validlabel) * 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fdaa847-bf10-4db7-86a5-a824696708f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(bit_width, model, save_dir):\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    filename = os.path.join(save_dir, f'ModelParameter_{bit_width}.pth')\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    logging.info(f\"Model saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7653d0c3-0b09-47ed-af63-3d9f270a4aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class var:\n",
    "    batch_size=32\n",
    "    nclass = len(open(os.path.join('data', 'ag_news', 'classes.txt'), 'r').readlines())\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "var=var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db1b0729-6be0-4582-8762-8c539aa233dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| Test | test loss  7170.726809718028  | test acc  88.4342105263158\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_start_time = time.time()\n",
    "test_loss, test_acc = evaluate(testdata, testlabel, model, criterion, var)\n",
    "test_time=time.time()-test_start_time\n",
    "print('=' * 89)\n",
    "print('| Test | test loss ', test_loss, ' | test acc ', test_acc)\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a2c60cf-a73b-49f3-a3bd-6115cbc40837",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = GenQLSTM(vocab_size, word_emb_dim, label_emb_dim, hid_dim, nlayers, nclass, dropout, use_cuda, tied, use_bias, concat_label, avg_loss, one_hot, bit_width).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82fbd368-72af-4071-a8bf-29c456f9bc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.weight\n",
      "label_encoder.weight\n",
      "rnn.layers.0.0.cell.output_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.cell_state_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.input_acc_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.forget_acc_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.cell_acc_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.output_acc_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.input_sigmoid_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.forget_sigmoid_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.cell_tanh_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.output_sigmoid_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.cell.hidden_state_tanh_quant.fused_activation_quant_proxy.tensor_quant.scaling_impl.value\n",
      "rnn.layers.0.0.input_gate_params.bias\n",
      "rnn.layers.0.0.input_gate_params.input_weight.weight\n",
      "rnn.layers.0.0.input_gate_params.input_weight.weight_quant.tensor_quant.scaling_impl.parameter_list_stats.extra_tracked_params_list.0.parameter\n",
      "rnn.layers.0.0.forget_gate_params.bias\n",
      "rnn.layers.0.0.forget_gate_params.input_weight.weight\n",
      "rnn.layers.0.0.forget_gate_params.input_weight.weight_quant.tensor_quant.scaling_impl.parameter_list_stats.extra_tracked_params_list.0.parameter\n",
      "rnn.layers.0.0.cell_gate_params.bias\n",
      "rnn.layers.0.0.cell_gate_params.input_weight.weight\n",
      "rnn.layers.0.0.cell_gate_params.input_weight.weight_quant.tensor_quant.scaling_impl.parameter_list_stats.extra_tracked_params_list.0.parameter\n",
      "rnn.layers.0.0.output_gate_params.bias\n",
      "rnn.layers.0.0.output_gate_params.input_weight.weight\n",
      "rnn.layers.0.0.output_gate_params.input_weight.weight_quant.tensor_quant.scaling_impl.parameter_list_stats.extra_tracked_params_list.0.parameter\n",
      "decoder.weight\n"
     ]
    }
   ],
   "source": [
    "for name, param in quantized_model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f288dbbf-b82b-438b-9ab6-dd78f08c1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the hidden_dim is defined based on your model configuration\n",
    "hidden_dim = hid_dim\n",
    "\n",
    "# Transfer parameters from pretrained model to quantized model\n",
    "quantized_model.encoder.weight.data = model.encoder.weight.data\n",
    "quantized_model.label_encoder.weight.data = model.label_encoder.weight.data\n",
    "\n",
    "# LSTM weights and biases\n",
    "quantized_model.rnn.layers[0][0].input_gate_params.input_weight.weight.data = model.rnn.weight_ih_l0[:hidden_dim, :].data\n",
    "quantized_model.rnn.layers[0][0].forget_gate_params.input_weight.weight.data = model.rnn.weight_ih_l0[hidden_dim:2*hidden_dim, :].data\n",
    "quantized_model.rnn.layers[0][0].cell_gate_params.input_weight.weight.data = model.rnn.weight_ih_l0[2*hidden_dim:3*hidden_dim, :].data\n",
    "quantized_model.rnn.layers[0][0].output_gate_params.input_weight.weight.data = model.rnn.weight_ih_l0[3*hidden_dim:, :].data\n",
    "\n",
    "quantized_model.rnn.layers[0][0].input_gate_params.hidden_weight.weight.data = model.rnn.weight_hh_l0[:hidden_dim, :].data\n",
    "quantized_model.rnn.layers[0][0].forget_gate_params.hidden_weight.weight.data = model.rnn.weight_hh_l0[hidden_dim:2*hidden_dim, :].data\n",
    "quantized_model.rnn.layers[0][0].cell_gate_params.hidden_weight.weight.data = model.rnn.weight_hh_l0[2*hidden_dim:3*hidden_dim, :].data\n",
    "quantized_model.rnn.layers[0][0].output_gate_params.hidden_weight.weight.data = model.rnn.weight_hh_l0[3*hidden_dim:, :].data\n",
    "\n",
    "quantized_model.rnn.layers[0][0].input_gate_params.bias.data = model.rnn.bias_ih_l0[:hidden_dim].data + model.rnn.bias_hh_l0[:hidden_dim].data\n",
    "quantized_model.rnn.layers[0][0].forget_gate_params.bias.data = model.rnn.bias_ih_l0[hidden_dim:2*hidden_dim].data + model.rnn.bias_hh_l0[hidden_dim:2*hidden_dim].data\n",
    "quantized_model.rnn.layers[0][0].cell_gate_params.bias.data = model.rnn.bias_ih_l0[2*hidden_dim:3*hidden_dim].data + model.rnn.bias_hh_l0[2*hidden_dim:3*hidden_dim].data\n",
    "quantized_model.rnn.layers[0][0].output_gate_params.bias.data = model.rnn.bias_ih_l0[3*hidden_dim:].data + model.rnn.bias_hh_l0[3*hidden_dim:].data\n",
    "\n",
    "quantized_model.decoder.weight.data = model.decoder.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1ef3357-fe6b-4711-b02c-0247b631b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir=f'./ModelParameter/FULL_Quantized/{bit_width}bit/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3dee2ff7-ca3a-4d36-ac34-2619a5d225ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fed597ba-e702-4763-b1ea-632845113134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    # Sort batch by sequence length in descending order\n",
    "    batch.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    sequences, labels = zip(*batch)\n",
    "    \n",
    "    # Pad sequences\n",
    "    sequences_padded = pad_sequence([torch.LongTensor(seq) for seq in sequences], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # Create a tensor for labels\n",
    "    labels_tensor = torch.LongTensor(labels)\n",
    "    \n",
    "    return sequences_padded, labels_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dfedbcf-75c1-4c3c-90db-dfcc39f3a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def get_calibration_dataloader(traindata, trainlabel, batch_size):\n",
    "    dataset = TextDataset(traindata, trainlabel)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    return dataloader\n",
    "\n",
    "# Creating DataLoader for calibration data\n",
    "calibration_dataloader = get_calibration_dataloader(traindata, trainlabel, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d79f0573-3c0c-4606-8600-8d889d84840d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_model(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            hidden = model.init_hidden(inputs.size(0))\n",
    "            \n",
    "            # Create y_ext manually to match input sequences\n",
    "            y_ext = torch.zeros_like(inputs)\n",
    "            for i, label in enumerate(labels):\n",
    "                y_ext[i, :] = label\n",
    "\n",
    "            # Shift x_pred to match the required prediction\n",
    "            x_pred = torch.zeros_like(inputs)\n",
    "            x_pred[:, :-1] = inputs[:, 1:]\n",
    "\n",
    "            _ = model(inputs, x_pred, y_ext, hidden, criterion=None, is_infer=True, cal=True)  # No criterion needed\n",
    "    print(\"Calibration complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4109436-3257-4084-b9f7-0b181a9d6286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass count: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:1271: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1788.)\n",
      "  return super().rename(names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass count: 2\n",
      "Forward pass count: 3\n",
      "Forward pass count: 4\n",
      "Forward pass count: 5\n",
      "Forward pass count: 6\n",
      "Forward pass count: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcalibrate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquantized_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibration_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 17\u001b[0m, in \u001b[0;36mcalibrate_model\u001b[0;34m(model, dataloader, device)\u001b[0m\n\u001b[1;32m     14\u001b[0m         x_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(inputs)\n\u001b[1;32m     15\u001b[0m         x_pred[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m inputs[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m---> 17\u001b[0m         _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_ext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_infer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# No criterion needed\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalibration complete.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/zxy-volume-ceph/mushfiqShovon/Gen-model/PTQ/models.py:201\u001b[0m, in \u001b[0;36mGenQLSTM.forward\u001b[0;34m(self, x, x_pred, y_ext, hidden, criterion, is_infer, cal)\u001b[0m\n\u001b[1;32m    196\u001b[0m embedded_sents \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(embedded_sents, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m#print(\"Embedded Sents Size after packing:\", type(embedded_sents), embedded_sents.size())\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m#print(\"hidden size:\", hidden.size())\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m output, hidden \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43membedded_sents\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted to handle RNN output\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m#print(\"output of rnn:\", output.data.size())\u001b[39;00m\n\u001b[1;32m    205\u001b[0m first_part \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata[:, \u001b[38;5;241m0\u001b[39m, :]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:979\u001b[0m, in \u001b[0;36mQuantLSTM.forward\u001b[0;34m(self, inp, hx, cx)\u001b[0m\n\u001b[1;32m    977\u001b[0m layer_hidden_state \u001b[38;5;241m=\u001b[39m hx[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l \u001b[38;5;241m+\u001b[39m d] \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[1;32m    978\u001b[0m layer_cell_state \u001b[38;5;241m=\u001b[39m cx[\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l \u001b[38;5;241m+\u001b[39m d] \u001b[38;5;28;01mif\u001b[39;00m cx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m cx\n\u001b[0;32m--> 979\u001b[0m out, out_hidden_state, out_cell_state \u001b[38;5;241m=\u001b[39m \u001b[43mdirection\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_hidden_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_cell_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m dir_outputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [out]\n\u001b[1;32m    981\u001b[0m dir_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [out_hidden_state]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:710\u001b[0m, in \u001b[0;36m_QuantLSTMLayer.forward\u001b[0;34m(self, inp, hidden_state, cell_state)\u001b[0m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    709\u001b[0m     cell \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\n\u001b[0;32m--> 710\u001b[0m quant_outputs, quant_hidden_state, quant_cell_state \u001b[38;5;241m=\u001b[39m \u001b[43mcell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_hidden_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_cell_state\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_ii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_ii\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_if\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_if\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_ic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_ic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_io\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_hi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_hi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_hf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_hf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_hc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_hc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_weight_ho\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_weight_ho\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_forget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_forget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_bias_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_bias_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    726\u001b[0m quant_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_quant_outputs(quant_outputs)\n\u001b[1;32m    727\u001b[0m quant_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpack_quant_state(quant_hidden_state, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcell\u001b[38;5;241m.\u001b[39moutput_quant)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:299\u001b[0m, in \u001b[0;36m_QuantLSTMCell.forward\u001b[0;34m(self, quant_input, quant_hidden_state, quant_cell_state, quant_weight_ii, quant_weight_if, quant_weight_ic, quant_weight_io, quant_weight_hi, quant_weight_hf, quant_weight_hc, quant_weight_ho, quant_bias_input, quant_bias_forget, quant_bias_cell, quant_bias_output)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end):\n\u001b[1;32m    298\u001b[0m     quant_input \u001b[38;5;241m=\u001b[39m quant_inputs[index]\n\u001b[0;32m--> 299\u001b[0m     quant_hidden_state_tuple, quant_cell_state_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_iter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_hidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_cell_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_ii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_if\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_ic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_hi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_hf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_hc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_weight_ho\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_forget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_cell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_bias_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     index \u001b[38;5;241m=\u001b[39m index \u001b[38;5;241m+\u001b[39m step\n\u001b[1;32m    316\u001b[0m     quant_hidden_states \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [quant_hidden_state_tuple]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/nn/quant_rnn.py:264\u001b[0m, in \u001b[0;36m_QuantLSTMCell.forward_iter\u001b[0;34m(self, quant_input, quant_hidden_state, quant_cell_state, quant_weight_ii, quant_weight_if, quant_weight_ic, quant_weight_io, quant_weight_hi, quant_weight_hf, quant_weight_hc, quant_weight_ho, quant_bias_input, quant_bias_forget, quant_bias_cell, quant_bias_output)\u001b[0m\n\u001b[1;32m    262\u001b[0m quant_hidden_state_tanh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_state_tanh_quant(quant_cell_state_tuple[\u001b[38;5;241m0\u001b[39m])[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    263\u001b[0m quant_hidden_state \u001b[38;5;241m=\u001b[39m quant_out_gate \u001b[38;5;241m*\u001b[39m quant_hidden_state_tanh\n\u001b[0;32m--> 264\u001b[0m quant_hidden_state_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquant_hidden_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m quant_hidden_state_tuple, quant_cell_state_tuple\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/proxy/runtime_quant.py:82\u001b[0m, in \u001b[0;36mFusedActivationQuantProxy.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_impl(x)\n\u001b[0;32m---> 82\u001b[0m     x, output_scale, output_zp, output_bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_quant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, output_scale, output_zp, output_bit_width\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/core/quant/int.py:152\u001b[0m, in \u001b[0;36mRescalingIntQuant.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;129m@brevitas\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mscript_method\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor]:\n\u001b[1;32m    151\u001b[0m     bit_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmsb_clamp_bit_width_impl()\n\u001b[0;32m--> 152\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     int_threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mint_scaling_impl(bit_width)\n\u001b[1;32m    154\u001b[0m     scale \u001b[38;5;241m=\u001b[39m threshold \u001b[38;5;241m/\u001b[39m int_threshold\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/core/scaling/standalone.py:356\u001b[0m, in \u001b[0;36mParameterFromRuntimeStatsScaling.forward\u001b[0;34m(self, stats_input)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    355\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 356\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mabs_binary_sign_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp_scaling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrestrict_scaling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/function/ops_ste.py:370\u001b[0m, in \u001b[0;36mabs_binary_sign_grad\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_tracing_state():\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mabs(x)\n\u001b[0;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn_prefix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd_ste_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs_binary_sign_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/brevitas/ops/autograd_ste_ops.py:372\u001b[0m, in \u001b[0;36mAbsBinarySignGradFn.forward\u001b[0;34m(ctx, x)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(ctx, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 372\u001b[0m     ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[43mbinary_sign\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint8\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# save some memory\u001b[39;00m\n\u001b[1;32m    373\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(x)\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "calibrate_model(quantized_model, calibration_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2cb0d-7110-436b-a2a2-d78284f6a4db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass count: 8\n",
      "Forward pass count: 9\n",
      "Forward pass count: 10\n",
      "Forward pass count: 11\n",
      "Forward pass count: 12\n",
      "Forward pass count: 13\n",
      "Forward pass count: 14\n",
      "Forward pass count: 15\n",
      "Forward pass count: 16\n",
      "Forward pass count: 17\n",
      "Forward pass count: 18\n",
      "Forward pass count: 19\n",
      "Forward pass count: 20\n",
      "Forward pass count: 21\n",
      "Forward pass count: 22\n",
      "Forward pass count: 23\n",
      "Forward pass count: 24\n",
      "Forward pass count: 25\n",
      "Forward pass count: 26\n",
      "Forward pass count: 27\n",
      "Forward pass count: 28\n",
      "Forward pass count: 29\n",
      "Forward pass count: 30\n",
      "Forward pass count: 31\n",
      "Forward pass count: 32\n",
      "Forward pass count: 33\n",
      "Forward pass count: 34\n",
      "Forward pass count: 35\n",
      "Forward pass count: 36\n",
      "Forward pass count: 37\n",
      "Forward pass count: 38\n",
      "Forward pass count: 39\n",
      "Forward pass count: 40\n",
      "Forward pass count: 41\n",
      "Forward pass count: 42\n",
      "Forward pass count: 43\n",
      "Forward pass count: 44\n",
      "Forward pass count: 45\n",
      "Forward pass count: 46\n",
      "Forward pass count: 47\n",
      "Forward pass count: 48\n",
      "Forward pass count: 49\n",
      "Forward pass count: 50\n",
      "Forward pass count: 51\n",
      "Forward pass count: 52\n",
      "Forward pass count: 53\n",
      "Forward pass count: 54\n",
      "Forward pass count: 55\n",
      "Forward pass count: 56\n",
      "Forward pass count: 57\n",
      "Forward pass count: 64\n",
      "Forward pass count: 65\n",
      "Forward pass count: 66\n",
      "Forward pass count: 67\n",
      "Forward pass count: 68\n",
      "Forward pass count: 69\n",
      "Forward pass count: 70\n",
      "Forward pass count: 71\n",
      "Forward pass count: 72\n",
      "Forward pass count: 73\n",
      "Forward pass count: 74\n",
      "Forward pass count: 75\n",
      "Forward pass count: 76\n",
      "Forward pass count: 77\n",
      "Forward pass count: 78\n",
      "Forward pass count: 79\n",
      "Forward pass count: 80\n",
      "Forward pass count: 81\n",
      "Forward pass count: 82\n",
      "Forward pass count: 83\n",
      "Forward pass count: 84\n",
      "Forward pass count: 85\n",
      "Forward pass count: 86\n",
      "Forward pass count: 87\n",
      "Forward pass count: 88\n",
      "Forward pass count: 89\n",
      "Forward pass count: 90\n",
      "Forward pass count: 91\n",
      "Forward pass count: 92\n",
      "Forward pass count: 93\n",
      "Forward pass count: 94\n",
      "Forward pass count: 95\n",
      "Forward pass count: 96\n",
      "Forward pass count: 97\n",
      "Forward pass count: 98\n",
      "Forward pass count: 99\n",
      "Forward pass count: 100\n",
      "Forward pass count: 101\n",
      "Forward pass count: 102\n",
      "Forward pass count: 103\n",
      "Forward pass count: 104\n",
      "Forward pass count: 105\n",
      "Forward pass count: 106\n",
      "Forward pass count: 107\n",
      "Forward pass count: 108\n",
      "Forward pass count: 109\n",
      "Forward pass count: 110\n",
      "Forward pass count: 111\n",
      "Forward pass count: 112\n",
      "Forward pass count: 113\n",
      "Forward pass count: 114\n",
      "Forward pass count: 115\n",
      "Forward pass count: 116\n",
      "Forward pass count: 117\n",
      "Forward pass count: 118\n",
      "Forward pass count: 119\n",
      "Forward pass count: 120\n",
      "Forward pass count: 121\n",
      "Forward pass count: 122\n",
      "Forward pass count: 123\n",
      "Forward pass count: 124\n",
      "Forward pass count: 125\n",
      "Forward pass count: 126\n",
      "Forward pass count: 127\n",
      "Forward pass count: 128\n",
      "Forward pass count: 129\n",
      "Forward pass count: 130\n",
      "Forward pass count: 131\n",
      "Forward pass count: 132\n",
      "Forward pass count: 133\n",
      "Forward pass count: 134\n",
      "Forward pass count: 135\n",
      "Forward pass count: 136\n",
      "Forward pass count: 137\n",
      "Forward pass count: 138\n",
      "Forward pass count: 139\n",
      "Forward pass count: 140\n",
      "Forward pass count: 141\n",
      "Forward pass count: 142\n",
      "Forward pass count: 143\n",
      "Forward pass count: 144\n",
      "Forward pass count: 145\n",
      "Forward pass count: 146\n",
      "Forward pass count: 147\n",
      "Forward pass count: 148\n",
      "Forward pass count: 149\n",
      "Forward pass count: 150\n",
      "Forward pass count: 151\n",
      "Forward pass count: 152\n",
      "Forward pass count: 153\n",
      "Forward pass count: 154\n",
      "Forward pass count: 155\n",
      "Forward pass count: 156\n",
      "Forward pass count: 157\n",
      "Forward pass count: 158\n",
      "Forward pass count: 159\n",
      "Forward pass count: 160\n",
      "Forward pass count: 161\n",
      "Forward pass count: 162\n",
      "Forward pass count: 163\n",
      "Forward pass count: 164\n",
      "Forward pass count: 165\n",
      "Forward pass count: 166\n",
      "Forward pass count: 167\n",
      "Forward pass count: 168\n",
      "Forward pass count: 169\n",
      "Forward pass count: 170\n",
      "Forward pass count: 171\n",
      "Forward pass count: 172\n",
      "Forward pass count: 173\n",
      "Forward pass count: 174\n",
      "Forward pass count: 175\n",
      "Forward pass count: 176\n",
      "Forward pass count: 177\n",
      "Forward pass count: 178\n",
      "Forward pass count: 179\n",
      "Forward pass count: 180\n",
      "Forward pass count: 181\n",
      "Forward pass count: 182\n",
      "Forward pass count: 183\n",
      "Forward pass count: 184\n",
      "Forward pass count: 185\n",
      "Forward pass count: 186\n",
      "Forward pass count: 187\n",
      "Forward pass count: 188\n",
      "Forward pass count: 189\n",
      "Forward pass count: 190\n",
      "Forward pass count: 191\n",
      "Forward pass count: 192\n",
      "Forward pass count: 193\n",
      "Forward pass count: 194\n",
      "Forward pass count: 195\n",
      "Forward pass count: 196\n",
      "Forward pass count: 197\n",
      "Forward pass count: 198\n",
      "Forward pass count: 199\n",
      "Forward pass count: 200\n",
      "Forward pass count: 201\n",
      "Forward pass count: 202\n",
      "Forward pass count: 203\n",
      "Forward pass count: 204\n",
      "Forward pass count: 205\n",
      "Forward pass count: 206\n",
      "Forward pass count: 207\n",
      "Forward pass count: 208\n",
      "Forward pass count: 209\n",
      "Forward pass count: 210\n",
      "Forward pass count: 211\n",
      "Forward pass count: 212\n",
      "Forward pass count: 213\n",
      "Forward pass count: 214\n",
      "Forward pass count: 215\n",
      "Forward pass count: 216\n",
      "Forward pass count: 217\n",
      "Forward pass count: 218\n",
      "Forward pass count: 219\n",
      "Forward pass count: 220\n",
      "Forward pass count: 221\n",
      "Forward pass count: 222\n",
      "Forward pass count: 223\n",
      "Forward pass count: 224\n",
      "Forward pass count: 225\n",
      "Forward pass count: 226\n",
      "Forward pass count: 227\n",
      "Forward pass count: 228\n",
      "Forward pass count: 229\n",
      "Forward pass count: 230\n",
      "Forward pass count: 231\n",
      "Forward pass count: 232\n",
      "Forward pass count: 233\n",
      "Forward pass count: 234\n",
      "Forward pass count: 235\n",
      "Forward pass count: 236\n",
      "Forward pass count: 237\n",
      "Forward pass count: 238\n",
      "Forward pass count: 239\n",
      "Forward pass count: 240\n",
      "Forward pass count: 241\n",
      "Forward pass count: 242\n",
      "Forward pass count: 243\n",
      "Forward pass count: 244\n",
      "Forward pass count: 245\n",
      "Forward pass count: 246\n",
      "Forward pass count: 247\n",
      "Forward pass count: 248\n",
      "Forward pass count: 249\n",
      "Forward pass count: 250\n",
      "Forward pass count: 251\n",
      "Forward pass count: 252\n",
      "Forward pass count: 253\n",
      "Forward pass count: 254\n",
      "Forward pass count: 255\n",
      "Forward pass count: 256\n",
      "Forward pass count: 257\n",
      "Forward pass count: 258\n",
      "Forward pass count: 259\n",
      "Forward pass count: 260\n",
      "Forward pass count: 261\n",
      "Forward pass count: 262\n",
      "Forward pass count: 263\n",
      "Forward pass count: 264\n",
      "Forward pass count: 265\n",
      "Forward pass count: 266\n",
      "Forward pass count: 267\n",
      "Forward pass count: 268\n",
      "Forward pass count: 269\n",
      "Forward pass count: 270\n",
      "Forward pass count: 271\n",
      "Forward pass count: 272\n",
      "Forward pass count: 273\n",
      "Forward pass count: 274\n",
      "Forward pass count: 275\n",
      "Forward pass count: 276\n",
      "Forward pass count: 277\n",
      "Forward pass count: 278\n",
      "Forward pass count: 279\n",
      "Forward pass count: 280\n",
      "Forward pass count: 281\n",
      "Forward pass count: 282\n",
      "Forward pass count: 283\n",
      "Forward pass count: 284\n",
      "Forward pass count: 285\n",
      "Forward pass count: 286\n",
      "Forward pass count: 287\n",
      "Forward pass count: 288\n",
      "Forward pass count: 289\n",
      "Forward pass count: 290\n",
      "Forward pass count: 291\n",
      "Forward pass count: 292\n",
      "Forward pass count: 293\n",
      "Forward pass count: 294\n",
      "Forward pass count: 295\n",
      "Forward pass count: 296\n",
      "Forward pass count: 297\n",
      "Forward pass count: 298\n",
      "Forward pass count: 299\n",
      "Forward pass count: 300\n",
      "Forward pass count: 301\n",
      "Forward pass count: 302\n",
      "Forward pass count: 303\n",
      "Forward pass count: 304\n",
      "Forward pass count: 305\n",
      "Forward pass count: 306\n",
      "Forward pass count: 307\n",
      "Forward pass count: 308\n",
      "Forward pass count: 309\n",
      "Forward pass count: 310\n",
      "Forward pass count: 311\n",
      "Forward pass count: 312\n",
      "Forward pass count: 313\n",
      "Forward pass count: 314\n",
      "Forward pass count: 315\n",
      "Forward pass count: 316\n",
      "Forward pass count: 317\n",
      "Forward pass count: 318\n",
      "Forward pass count: 319\n",
      "Forward pass count: 320\n",
      "Forward pass count: 321\n",
      "Forward pass count: 322\n",
      "Forward pass count: 323\n",
      "Forward pass count: 324\n",
      "Forward pass count: 325\n",
      "Forward pass count: 326\n",
      "Forward pass count: 327\n",
      "Forward pass count: 328\n",
      "Forward pass count: 329\n",
      "Forward pass count: 330\n",
      "Forward pass count: 331\n",
      "Forward pass count: 332\n",
      "Forward pass count: 333\n",
      "Forward pass count: 334\n",
      "Forward pass count: 335\n",
      "Forward pass count: 336\n",
      "Forward pass count: 337\n",
      "Forward pass count: 338\n",
      "Forward pass count: 339\n",
      "Forward pass count: 340\n",
      "Forward pass count: 341\n",
      "Forward pass count: 342\n",
      "Forward pass count: 343\n",
      "Forward pass count: 344\n",
      "Forward pass count: 345\n",
      "Forward pass count: 346\n",
      "Forward pass count: 347\n",
      "Forward pass count: 348\n",
      "Forward pass count: 349\n",
      "Forward pass count: 350\n",
      "Forward pass count: 351\n",
      "Forward pass count: 352\n",
      "Forward pass count: 353\n",
      "Forward pass count: 354\n",
      "Forward pass count: 355\n",
      "Forward pass count: 356\n",
      "Forward pass count: 357\n",
      "Forward pass count: 358\n",
      "Forward pass count: 359\n",
      "Forward pass count: 360\n",
      "Forward pass count: 361\n",
      "Forward pass count: 362\n",
      "Forward pass count: 363\n",
      "Forward pass count: 364\n",
      "Forward pass count: 365\n",
      "Forward pass count: 366\n",
      "Forward pass count: 367\n",
      "Forward pass count: 368\n",
      "Forward pass count: 369\n",
      "Forward pass count: 370\n",
      "Forward pass count: 371\n",
      "Forward pass count: 372\n",
      "Forward pass count: 373\n",
      "Forward pass count: 374\n",
      "Forward pass count: 375\n",
      "Forward pass count: 376\n",
      "Forward pass count: 377\n",
      "Forward pass count: 378\n",
      "Forward pass count: 379\n",
      "Forward pass count: 380\n",
      "Forward pass count: 381\n",
      "Forward pass count: 382\n",
      "Forward pass count: 383\n",
      "Forward pass count: 384\n",
      "Forward pass count: 385\n",
      "Forward pass count: 386\n",
      "Forward pass count: 387\n",
      "Forward pass count: 388\n",
      "Forward pass count: 389\n",
      "Forward pass count: 390\n",
      "Forward pass count: 391\n",
      "Forward pass count: 392\n",
      "Forward pass count: 393\n",
      "Forward pass count: 394\n",
      "Forward pass count: 395\n",
      "Forward pass count: 396\n",
      "Forward pass count: 397\n",
      "Forward pass count: 398\n",
      "Forward pass count: 399\n",
      "Forward pass count: 400\n",
      "Forward pass count: 401\n",
      "Forward pass count: 402\n",
      "Forward pass count: 403\n",
      "Forward pass count: 404\n",
      "Forward pass count: 405\n",
      "Forward pass count: 406\n",
      "Forward pass count: 407\n",
      "Forward pass count: 408\n",
      "Forward pass count: 409\n",
      "Forward pass count: 410\n",
      "Forward pass count: 411\n",
      "Forward pass count: 412\n",
      "Forward pass count: 413\n",
      "Forward pass count: 414\n",
      "Forward pass count: 415\n",
      "Forward pass count: 416\n",
      "Forward pass count: 417\n",
      "Forward pass count: 418\n",
      "Forward pass count: 419\n",
      "Forward pass count: 420\n",
      "Forward pass count: 421\n",
      "Forward pass count: 422\n",
      "Forward pass count: 423\n",
      "Forward pass count: 424\n",
      "Forward pass count: 425\n",
      "Forward pass count: 426\n",
      "Forward pass count: 427\n",
      "Forward pass count: 428\n",
      "Forward pass count: 429\n",
      "Forward pass count: 430\n",
      "Forward pass count: 431\n",
      "Forward pass count: 432\n",
      "Forward pass count: 433\n",
      "Forward pass count: 434\n",
      "Forward pass count: 435\n",
      "Forward pass count: 436\n",
      "Forward pass count: 437\n",
      "Forward pass count: 438\n",
      "Forward pass count: 439\n",
      "Forward pass count: 440\n",
      "Forward pass count: 441\n"
     ]
    }
   ],
   "source": [
    "quantized_model.eval()\n",
    "test_start_time = time.time()\n",
    "test_loss, test_acc = evaluate(testdata, testlabel, quantized_model, criterion, var)\n",
    "test_time=time.time()-test_start_time\n",
    "print('=' * 89)\n",
    "print('| Test | test loss ', test_loss, ' | test acc ', test_acc)\n",
    "print('=' * 89)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca3b8e-f359-4fa8-a1a2-7f4d70e8bc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(0, quantized_model, model_dir)\n",
    "accuracies = []\n",
    "accuracies.append({'Epoch': 'Test', 'Validation Accuracy': test_acc, 'Epoch Time': test_time})\n",
    "df_accuracies = pd.DataFrame(accuracies)\n",
    "accuracy_save_path = model_dir + f'accuracies_{bit_width}bit.csv'\n",
    "df_accuracies.to_csv(accuracy_save_path, index=False)\n",
    "print(f'Accuracies saved at \"{accuracy_save_path}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
